{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d12b9c0-1964-4785-a58a-565d38305d87",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "![Generating Embeddings](../../images/headings/02_retrieval_augmented_generation_02_01_prompts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7baea3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Prompts: Langchain\n",
    "\n",
    "This notebook demonstrates how to use `PromptTemplate` from the `langchain_core` package. `PromptTemplate` tooling allows you to pre-design the form and content of text-based instructions sent to Language Models (LMs) during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e79a1",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b940df9",
   "metadata": {},
   "source": [
    "### OpenAI messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f00728",
   "metadata": {},
   "source": [
    "**OpenAI conversations**\n",
    "_definitions modified from: https://cdn.openai.com/spec/model-spec-2024-05-08.html_\n",
    "\n",
    "**Assistant:** the entity that the end user or developer interacts with. While language models can generate text continuations of any input, the models we're using today have been fine-tuned on inputs formatted as conversations, consisting of a list of messages. In these conversations, the model is only designed to play one participant, called the assistant. In this document, when we discuss model behavior, we're referring to its behavior as the assistant; \"model\" and \"assistant\" will be approximately synonymous.\n",
    "\n",
    "**Conversation:** valid input to the model is a conversation, which consists of a list of messages.\n",
    "\n",
    "Example message:\n",
    "```json\n",
    "{\n",
    "    \"role\": \"assistant\",\n",
    "    \"recipient\": \"browser\",\n",
    "    \"content\": \"assist the user\",\n",
    "    \"end_turn\": true,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff7ca1",
   "metadata": {},
   "source": [
    "### A brief note about tokens\n",
    "\n",
    "When you send the message to the LM, the request becomes a single string jammed with special tokens like so:\n",
    "```\n",
    "<|start|>assistant<|recipient|>browser<|content|>assist the user<|end_turn|>\n",
    "```\n",
    "\n",
    "where `<|...|>` denotes a special token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7dc3f3",
   "metadata": {},
   "source": [
    "### Claude conversations\n",
    "\n",
    "_see https://docs.anthropic.com/claude/reference/messages_post for more information_\n",
    "\n",
    "Example conversation:\n",
    "```json\n",
    "{\n",
    "    \"model\": \"claude-3-opus-20240229\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"system\": \"You are a helpful assistant...\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Hello, world\"}\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3460a85b",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1475e41b-17df-4974-bcbf-be0b12d101fc",
   "metadata": {},
   "source": [
    "### Templates can interpolate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87404894",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"{user} wants to hear a joke about: \\\"{content}\\\".\"\n",
    ")\n",
    "prompt_template.format(user=\"Brendan\", content=\"stage fright\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1668e44-3f65-4055-a76f-ae1c3e014d95",
   "metadata": {},
   "source": [
    "### Templates without variables\n",
    "This works just fine, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edef879",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me about your favorite library.\"\n",
    ")\n",
    "prompt_template.format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589fad90-7629-41dc-bc5b-8db82e8eae45",
   "metadata": {},
   "source": [
    "### System Prompts and Human Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa240c-8f2c-48b9-b4ae-bbe9fe57e7c1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that provides information about \"\n",
    "                \"library services and facilitates access to library resources.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_val = chat_template.invoke({\"text\": \"How can I request books from other libraries besides this one?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_val.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274c46e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_val.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a9f16",
   "metadata": {},
   "source": [
    "## Advanced Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea3ed23-8545-4405-bc2b-094d09f98dcf",
   "metadata": {},
   "source": [
    "### Building prompts dynamically\n",
    "\n",
    "This example is taken directly from the Northwestern University Library Digital Collections AI search prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a7cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "def prompt_template() -> str:\n",
    "    return \"\"\"Please provide an answer to the question based on the documents provided. Include specific details from the documents that support your answer. Each document is identified by a 'title' and a unique 'source' UUID:\n",
    "\n",
    "Documents:\n",
    "{context}\n",
    "Answer in raw markdown. When referencing a document by title, link to it using its UUID like this: [title](https://dc.library.northwestern.edu/items/UUID). For example: [Judy Collins, Jackson Hole Folk Festival](https://dc.library.northwestern.edu/items/f1ca513b-7d13-4af6-ad7b-8c7ffd1d3a37). Suggest keyword searches using this format: [keyword](https://dc.library.northwestern.edu/search?q=keyword). Offer a variety of search terms that cover different aspects of the topic. Include as many direct links to Digital Collections searches as necessary for a thorough study. The `collection` field contains information about the collection the document belongs to. In the summary, mention the top 1 or 2 collections, explain why they are relevant and link to them using the collection title and id: [collection['title']](https://dc.library.northwestern.edu/collections/collection['id']), for example [World War II Poster Collection](https://dc.library.northwestern.edu/collections/faf4f60e-78e0-4fbf-96ce-4ca8b4df597a):\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "def document_template(attributes: Optional[List[str]] = None) -> str:\n",
    "    if attributes is None:\n",
    "        attributes = []\n",
    "    lines = (\n",
    "        [\"Content: {title}\", \"Metadata:\"]\n",
    "        + [f\"  {attribute}: {{{attribute}}}\" for attribute in attributes]\n",
    "        + [\"Source: {id}\"]\n",
    "    )\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a0bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = document_template(attributes=[\"collection\", \"date\", \"description\", \"format\", \"language\", \"rights\", \"subject\", \"type\"])\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_template())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9e03a-e43a-4667-9e0d-390070bdf3c1",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544457aa-d28d-45da-8823-e248fb99bdf3",
   "metadata": {},
   "source": [
    "- Create prompts for a few real or imagined scenarios. How would you want to steer a language model on behalf of your users?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0372f6f-9565-448a-aac9-df49b87ddc29",
   "metadata": {},
   "source": [
    "## Discussion Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce5769-561f-48c5-a821-24e0ce5fea3f",
   "metadata": {},
   "source": [
    "- This langchain business seems complicated, do the abstractions provided by the framework help you with these concepts?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
